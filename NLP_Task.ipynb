{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation **for Google Colab**"
      ],
      "metadata": {
        "id": "34tb5DWDiZaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For first time installing CAMel tools > use the installation code at the end of this colab\n",
        "#For CAMel tools\n",
        "%pip install camel-tools\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/gdrive')\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'"
      ],
      "metadata": {
        "id": "IENCEhrsiP5O"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for Wikipedia API\n",
        "!pip3 install wikipedia-api"
      ],
      "metadata": {
        "id": "9OXfzV20iupc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for nltk\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "p28kfJhIuSe4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task"
      ],
      "metadata": {
        "id": "ulEfIseQi_hi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi #to extract articles\n",
        "import re #regular expression\n",
        "#for POS tagging\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tagger.default import DefaultTagger\n",
        "#for NER recognition\n",
        "from camel_tools.ner import NERecognizer\n",
        "#for normalization\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "#for stemmer\n",
        "from nltk.stem.isri import ISRIStemmer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Review two Python NLP libraries\n",
        "## NLTK vs. CAMeL\n",
        "### Natural Language Processing Libraries in Python"
      ],
      "metadata": {
        "id": "hDF5g_49Mplu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used to work with [NLTK](https://link-url-here.org) (Natural Language Toolkit), a free, open source, famous toolkit in Python that provides a suite of text processing libraries for classification, tokenization, stemming, tagging and other NLP tasks. Some of its functions and libraries are supporting Arabic, but the others are not. \n",
        "\n",
        "When I start doing this task I faced some issues regarding Arabic functions. That's why I decided to look for another useful toolkit to perform this task.\n",
        "\n",
        "[CAMeL tools](https://camel-tools.readthedocs.io/en/latest/) is a suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi. It provides utilities for pre-processing, POS tagging, dialect identification, named entity recognition and sentiment analysis. \n",
        "\n",
        "Unfortunately, I couldn't run POS tagging due to some security limits by Google to access huge files on the drive (I'm using Google Colab and the data files are on Google Drive).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IYNtdxuHNDSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenise, 3.Preprocessing, 4. Tagging"
      ],
      "metadata": {
        "id": "Gxp19kMJ8oMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract data"
      ],
      "metadata": {
        "id": "9IscFV7FhQ8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wikipedia objects to extract articles\n",
        "wiki1=wikipediaapi.Wikipedia(\"ar\")  # ar refers to Modern Standard Arabic version of Wikipedia\n",
        "wiki2=wikipediaapi.Wikipedia(\"arz\") # arz refers to Egyption dialect version of Wikipedia"
      ],
      "metadata": {
        "id": "lRXo53-DbI4k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get pages based on their titles\n",
        "try:\n",
        "    page1 = wiki1.page('لهجة مصرية')\n",
        "    page2 = wiki2.page('اللغه المصريه الحديثه')\n",
        "    \n",
        "except:\n",
        "    print(\"An exception occurred\")"
      ],
      "metadata": {
        "id": "MTK_Km9NpzeB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the first 200 characters of the two articles\n",
        "print('Article 1:\\n',page1.summary[0:200])\n",
        "print('Article 2:\\n',page2.summary[0:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbMVGJMop3cb",
        "outputId": "8820f887-53e2-4d0a-be00-fa54cb3d5f2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            " اللهجة العربية المصرية، المعروفة محليًا باسم العامية المصرية أو المصري، يتحدث بها معظم المصريين المعاصرين.\n",
            "المصرية هي لهجة شمال إفريقية للغة العربية وهي فرع سامي من عائلة اللغات الإفروآسيوية. نشأت في \n",
            "Article 2:\n",
            " اللغه المصريه الحديثه او المصرى هى اللغه اللى المصريين بيتكلّموها فى مصر, ابتدا تاريخها فى دلتا النيل حوالين بلادها الحضريه زى القاهره واسكندريه.\n",
            "النهارده اللغه المصرى هى اللغه السايده فى مصر وبتتكون \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract text from the two pages\n",
        "article1=page1.text\n",
        "article2=page2.text\n",
        "text=[article1,article2]"
      ],
      "metadata": {
        "id": "WNUV5syOqNr0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special characters, punctuations, english letters, and digits removing"
      ],
      "metadata": {
        "id": "xF-gw_LaavVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text=[]\n",
        "for article in text:\n",
        "    article=re.sub(r'[^\\w\\s]','',article)   #Remove special characters & punctuations\n",
        "    article=re.sub(r'[a-zA-Z\\d]','',article) #Remove english letters & digits\n",
        "    cleaned_text.append(article)"
      ],
      "metadata": {
        "id": "dyf2zEecqVY4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display first 300 letters of the two articles\n",
        "print('Article 1:\\n',cleaned_text[0][0:300])\n",
        "print('Article 2:\\n',cleaned_text[1][0:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX9P1gjgqVlu",
        "outputId": "aea01cb6-ca9c-4d5b-e6d3-4f8b8c6e6033"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            " اللهجة العربية المصرية المعروفة محليا باسم العامية المصرية أو المصري يتحدث بها معظم المصريين المعاصرين\n",
            "المصرية هي لهجة شمال إفريقية للغة العربية وهي فرع سامي من عائلة اللغات الإفروآسيوية نشأت في دلتا النيل في مصر السفلى حول العاصمة القاهرة تطورت اللهجة المصرية من اللغة العربية التي تم نقلها إلى مصر \n",
            "Article 2:\n",
            " اللغه المصريه الحديثه او المصرى هى اللغه اللى المصريين بيتكلموها فى مصر ابتدا تاريخها فى دلتا النيل حوالين بلادها الحضريه زى القاهره واسكندريه\n",
            "النهارده اللغه المصرى هى اللغه السايده فى مصر وبتتكون من لهجات زى مصرى قاهراوى  بحيرى او مصرى دلتاوى فى جنوب مصر الناس بتتكلم مصري صعيدى و هى لغه بنفسيها معت\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenizing & Stopwords Removing\n",
        "\n"
      ],
      "metadata": {
        "id": "e53odOhvbpKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Stopwords text file from: https://github.com/mohataher/arabic-stop-words\n",
        "#Read the file\n",
        "stopwords=[]\n",
        "with open('/gdrive/MyDrive/ColabNotebooks/list.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "#remove \\n from each line\n",
        "for i in range(len(lines)):\n",
        "  stopwords.append(lines[i].rstrip('\\n'))\n",
        "#Print some of the stopwords\n",
        "print(stopwords[100:110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GmvqsiTMlQA",
        "outputId": "9bdc97c8-ab83-4eaa-efa0-8487891a84d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إليكم', 'إليكما', 'إليكنّ', 'إليكَ', 'إلَيْكَ', 'إلّا', 'إمّا', 'إن', 'إنَّ', 'إى']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Each article will be tokenized and stopwords will be reomoved\n",
        "filtered_tokens=[]\n",
        "filtered_articles=[]\n",
        "for article in cleaned_text:\n",
        "    #Tokenize\n",
        "    tokens=article.split()\n",
        "    #Check if any word is a stopword, to be neglected\n",
        "    for word in tokens:\n",
        "        if word not in stopwords:\n",
        "            filtered_tokens.append(word)\n",
        "    #Add tokenized article to the article list\n",
        "    filtered_articles.append(filtered_tokens)\n",
        "    filtered_tokens=[]"
      ],
      "metadata": {
        "id": "SyX67-Jnb1ay"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of the egyption article after tokenizing and stopwords removing\n",
        "print(filtered_articles[1][0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L7OHNoVdDZz",
        "outputId": "086c1e28-5361-415c-de73-5a69f2c23a7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['اللغه', 'المصريه', 'الحديثه', 'المصرى', 'هى', 'اللغه', 'اللى', 'المصريين', 'بيتكلموها', 'مصر', 'ابتدا', 'تاريخها', 'دلتا', 'النيل', 'حوالين', 'بلادها', 'الحضريه', 'زى', 'القاهره', 'واسكندريه']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-Speech and Named entity recognition tagging using CAMeL tools"
      ],
      "metadata": {
        "id": "y9EmgSCEhjYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POS Tagging"
      ],
      "metadata": {
        "id": "2IFA55rNfoYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ALERT: THIS CELL IS NOT WORKING\n",
        "#The issue only appears on Colab and it seems to be that a Google Drive is blocking the download since the file is too big and thus the antivirus cannot be applied:\n",
        "#FileNotFoundError: [Errno 2] No such file or directory: '/gdrive/MyDrive/camel_tools/data/morphology_db/calima-egy-r13/morphology.db'\n",
        "\n",
        "#Pretrained tagger for MSA \n",
        "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "#Pretrained tagger for Egyption Arabic \n",
        "mle2 = MLEDisambiguator.pretrained('calima-egy-r13')\n",
        "\n",
        "tagger = DefaultTagger(mle, 'pos')\n",
        "tagger2 = DefaultTagger(mle2, 'pos')\n",
        "\n",
        "pos_tags_article1 = tagger.tag(filtered_articles[0])\n",
        "pos_tags_article2 = tagger2.tag(filtered_articles[1])\n",
        "\n",
        "#Print part of POS tags for MSA article\n",
        "print(pos_tags_article1[0:20])\n",
        "\n",
        "#Print part of POS tags for Egyption article\n",
        "print(pos_tags_article2[0:20])"
      ],
      "metadata": {
        "id": "jOtiGtWrc2vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NER Tagging"
      ],
      "metadata": {
        "id": "BKk20AJguMt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition tags\n",
        "#Pretrained model\n",
        "ner = NERecognizer.pretrained()\n",
        "#Tags\n",
        "ner_tags=[]\n",
        "for article in filtered_articles:\n",
        "  labels = ner.predict_sentence(article)\n",
        "  ner_tags.append(list(zip(article, labels)))"
      ],
      "metadata": {
        "id": "qRguv6Ryng3o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of token-NER label pairs for article 1\n",
        "print('Article 1 NER:\\n',ner_tags[0][0:20])\n",
        "#Print part of token-NER label pairs for article 2\n",
        "print('Article 2 NER:\\n',ner_tags[1][0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqPmyEN1s0Nv",
        "outputId": "95eb61a8-9272-41f6-8f42-7162c1bc85ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1 NER:\n",
            " [('اللهجة', 'O'), ('العربية', 'O'), ('المصرية', 'O'), ('المعروفة', 'O'), ('محليا', 'O'), ('العامية', 'O'), ('المصرية', 'O'), ('المصري', 'O'), ('يتحدث', 'O'), ('معظم', 'O'), ('المصريين', 'O'), ('المعاصرين', 'O'), ('المصرية', 'O'), ('لهجة', 'O'), ('إفريقية', 'O'), ('للغة', 'O'), ('العربية', 'O'), ('فرع', 'O'), ('سامي', 'O'), ('عائلة', 'O')]\n",
            "Article 2 NER:\n",
            " [('اللغه', 'O'), ('المصريه', 'O'), ('الحديثه', 'O'), ('المصرى', 'O'), ('هى', 'O'), ('اللغه', 'O'), ('اللى', 'O'), ('المصريين', 'O'), ('بيتكلموها', 'O'), ('مصر', 'B-LOC'), ('ابتدا', 'O'), ('تاريخها', 'O'), ('دلتا', 'O'), ('النيل', 'I-LOC'), ('حوالين', 'O'), ('بلادها', 'O'), ('الحضريه', 'O'), ('زى', 'O'), ('القاهره', 'B-LOC'), ('واسكندريه', 'B-LOC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Find related open class words"
      ],
      "metadata": {
        "id": "8g15GCTEiN-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing Alef & Alef_maksura & teh_marbuta\n",
        "\n",
        "MSA applies Hamza of Alef hamza, teh marbuta, but Egyption Arabic does not use teh marbuta and no Hamza for Alef. The normalization is done to make the words more similar "
      ],
      "metadata": {
        "id": "Twlhk3MPTnI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize for Standard Arabic\n",
        "norm_article=[]\n",
        "norm_articles=[]\n",
        "\n",
        "for word in filtered_articles[0]:\n",
        "  #Normalize alef variants to 'ا'\n",
        "  word_norm = normalize_alef_ar(word)\n",
        "  #Normalize teh marbuta 'ة' to heh 'ه'\n",
        "  word_norm = normalize_teh_marbuta_ar(word_norm)\n",
        "  norm_article.append(word_norm)\n",
        "\n",
        "#Add the article after normalization to the list of normalized articles\n",
        "norm_articles.append(norm_article)\n",
        "norm_article=[]\n",
        "\n",
        "#Normailze for Egyption Arabic\n",
        "for word in filtered_articles[1]:\n",
        "  #Normalize alef maksura 'ى' to yeh 'ي'\n",
        "  word_norm = normalize_alef_maksura_ar(word)\n",
        "  norm_article.append(word_norm)\n",
        "\n",
        "#Add the article after normalization to the list of normalized articles\n",
        "norm_articles.append(norm_article)"
      ],
      "metadata": {
        "id": "-11HKy2WqGYp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the first 10 words in each article before and after normalization to show the difference\n",
        "print('Article 1:\\n',filtered_articles[0][0:10])\n",
        "print(norm_articles[0][0:10])\n",
        "print('*'*60)\n",
        "print('Article 2:\\n',filtered_articles[1][0:10])\n",
        "print(norm_articles[1][0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yz8CrBJlrbi",
        "outputId": "bb2a40ee-800c-4ca1-df17-1c4e10985335"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            " ['اللهجة', 'العربية', 'المصرية', 'المعروفة', 'محليا', 'العامية', 'المصرية', 'المصري', 'يتحدث', 'معظم']\n",
            "['اللهجه', 'العربيه', 'المصريه', 'المعروفه', 'محليا', 'العاميه', 'المصريه', 'المصري', 'يتحدث', 'معظم']\n",
            "************************************************************\n",
            "Article 2:\n",
            " ['اللغه', 'المصريه', 'الحديثه', 'المصرى', 'هى', 'اللغه', 'اللى', 'المصريين', 'بيتكلموها', 'مصر']\n",
            "['اللغه', 'المصريه', 'الحديثه', 'المصري', 'هي', 'اللغه', 'اللي', 'المصريين', 'بيتكلموها', 'مصر']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are similar words?\n",
        "I can answer this in two ways:\n",
        "\n",
        "**First: Synonyms**. Synonyms cannot be found using the existing Arabic NLP libraries -to my knowledge-, but we can achieve this task using a pre-trained embedding model or train any embedding model from scratch after transforming the Arabic text into embeddings (vector of numbers), then allow the model to calculate the distance between the words to find most similar words. I did an expriment using a pre-trained word2vec embedding model that was trained using Wikipedia Arabic articles. Unfortunately, all the similar words the model found were different inflections of words, not synonyms. \n",
        "\n",
        "**Second: Inflections**. Inflection is a process of word formation. We can return words to their root or lemma using lammtizating or stemming, then try to find similar roots between the two articles. This is faster than using embedding models.\n"
      ],
      "metadata": {
        "id": "uQkr6qEtilHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Since POS tagging didn't work on Colab, thus, I didn't use NER tags either to find similar words. I used an Arabic stemmer in NLTK library since there is not any stemmer in CAMeL tools.**"
      ],
      "metadata": {
        "id": "_flLWwM6k99q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming to find similarities\n"
      ],
      "metadata": {
        "id": "9KegjPpxmvZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the stemmer\n",
        "stemmer = ISRIStemmer()\n",
        "similar_pairs=[]\n",
        "article1=list(set(norm_articles[0]))\n",
        "article2=list(set(norm_articles[1]))\n",
        "for i in range(0,len(article1)):\n",
        "  for w in range(0,len(article2)):\n",
        "    if stemmer.stem(article1[i]) == stemmer.stem(article2[w]):\n",
        "      similar_pairs.append([i,w])"
      ],
      "metadata": {
        "id": "45luR28dptcu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print no. of similar pairs\n",
        "print(len(similar_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mizgTRmXvNbG",
        "outputId": "96d9ee65-69a3-4071-d4ae-b9555da17e4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of the similar pairs\n",
        "print(similar_pairs[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdJdeUk5vh2Z",
        "outputId": "bd69af36-5f83-4957-8e71-faeedef69000"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 1306], [3, 1061], [4, 3], [4, 205], [4, 308], [4, 566], [4, 742], [4, 896], [4, 1119], [4, 1244]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of the similar words between the two articles\n",
        "print(\"Article 1 _______ Article 2\")\n",
        "for i in range(130,150):\n",
        "  print(article1[similar_pairs[i][0]],' _______ ',article2[similar_pairs[i][1]])\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r6e0HOLzBp7",
        "outputId": "2a3d92b1-0bc8-47f1-9dbf-5dcffe7f9427"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1 _______ Article 2\n",
            "اللغات  _______  لغتهم\n",
            "اللغات  _______  لغته\n",
            "صناعه  _______  مصنوعة\n",
            "الشام  _______  الشام\n",
            "رئيسي  _______  رئيس\n",
            "كاللغه  _______  باللغه\n",
            "كاللغه  _______  لغه\n",
            "كاللغه  _______  لللغه\n",
            "كاللغه  _______  اللغه\n",
            "الراجل  _______  الرجاله\n",
            "مدارس  _______  دراسه\n",
            "مدارس  _______  المدرسه\n",
            "مدارس  _______  بتدرس\n",
            "مدارس  _______  درس\n",
            "مدارس  _______  الدرس\n",
            "مدارس  _______  مدرسين\n",
            "مدارس  _______  ادريس\n",
            "مدارس  _______  المدارس\n",
            "انظروا  _______  النظر\n",
            "انظروا  _______  نظرية\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These 3 cells we run in first time using this notebook on Colab only:"
      ],
      "metadata": {
        "id": "jRKIfPYo0Uic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-tools -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "xItbltssY2Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-tools -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%mkdir /gdrive/MyDrive/camel_tools"
      ],
      "metadata": {
        "id": "R43D9c_g0up8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install camel tools data\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n",
        "#!export | camel_data -i all\n",
        "!export | camel_data -i defaults"
      ],
      "metadata": {
        "id": "plbhDr1O0wg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jRKIfPYo0Uic"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}