{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation **for Google Colab**"
      ],
      "metadata": {
        "id": "34tb5DWDiZaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For first time installing CAMel tools > use the installation code at the end of this colab\n",
        "#For CAMel tools\n",
        "%pip install camel-tools\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/gdrive')\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IENCEhrsiP5O",
        "outputId": "b2c6187e-802d-4f14-d903-e56bbb9535e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.4.1-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from camel-tools) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from camel-tools) (1.22.4)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from camel-tools) (0.16.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting camel-kenlm\n",
            "  Downloading camel-kenlm-2021.12.27.tar.gz (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.2/418.2 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from camel-tools) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from camel-tools) (1.3.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from camel-tools) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from camel-tools) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.8/dist-packages (from camel-tools) (1.13.1+cu116)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.8/dist-packages (from camel-tools) (5.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from camel-tools) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from camel-tools) (2.25.1)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.8/dist-packages (from camel-tools) (0.19.3)\n",
            "Collecting transformers>=3.0.2\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from camel-tools) (0.8.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from camel-tools) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->camel-tools) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel-tools) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel-tools) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel-tools) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel-tools) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->camel-tools) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->camel-tools) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->camel-tools) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->camel-tools) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->camel-tools) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->camel-tools) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->camel-tools) (1.2.0)\n",
            "Building wheels for collected packages: camel-kenlm, docopt, emoji\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2021.12.27-cp38-cp38-linux_x86_64.whl size=2970430 sha256=56db00f701a9b0bfc559a4a25bde89dc74418719ce0fad42adf314010ff1588e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/7b/f0/837fcdb48cd99564b1163d90392f350cb933fce3bf122eadcd\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=1b7e2085c86d0d68082061dcc4eb76fcbfe7a8e55b3f3661fad9fb2505877553\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=983b9672276123b2f1f07ce72b82f032e6b959a7e5dbe035e094bcaee5593866\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built camel-kenlm docopt emoji\n",
            "Installing collected packages: tokenizers, docopt, camel-kenlm, emoji, huggingface-hub, transformers, camel-tools\n",
            "Successfully installed camel-kenlm-2021.12.27 camel-tools-1.4.1 docopt-0.6.2 emoji-2.2.0 huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for Wikipedia API\n",
        "!pip3 install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OXfzV20iupc",
        "outputId": "7fe02a82-761a-4b04-b20c-b9e8d88ecbff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from wikipedia-api) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (2022.12.7)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for nltk\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p28kfJhIuSe4",
        "outputId": "15d458e4-d24e-4409-e408-d649da77cf60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task"
      ],
      "metadata": {
        "id": "ulEfIseQi_hi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi #to extract articles\n",
        "import re #regular expression\n",
        "#for POS tagging\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tagger.default import DefaultTagger\n",
        "#for NER recognition\n",
        "from camel_tools.ner import NERecognizer\n",
        "#for normalization\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "#for stemmer\n",
        "from nltk.stem.isri import ISRIStemmer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Review two Python NLP libraries\n",
        "## NLTK vs. CAMeL\n",
        "### Natural Language Processing Libraries in Python"
      ],
      "metadata": {
        "id": "hDF5g_49Mplu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used to work with [NLTK](https://link-url-here.org) (Natural Language Toolkit), a free, open source, famous toolkit in Python that provides a suite of text processing libraries for classification, tokenization, stemming, tagging and other NLP tasks. Some of its functions and libraries are supporting Arabic, but the others are not. \n",
        "\n",
        "When I start doing this task I faced some issues regarding Arabic functions. That's why I decided to look for another useful toolkit to perform this task.\n",
        "\n",
        "[CAMeL tools](https://camel-tools.readthedocs.io/en/latest/) is a suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi. It provides utilities for pre-processing, POS tagging, dialect identification, named entity recognition and sentiment analysis. \n",
        "\n",
        "Unfortunately, I couldn't run POS tagging due to some security limits by Google to access huge files on the drive (I'm using Google Colab and the data files are on Google Drive).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IYNtdxuHNDSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenise, 3.Preprocessing, 4. Tagging"
      ],
      "metadata": {
        "id": "Gxp19kMJ8oMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract data"
      ],
      "metadata": {
        "id": "9IscFV7FhQ8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wikipedia objects to extract articles\n",
        "wiki1=wikipediaapi.Wikipedia(\"ar\")  # ar refers to Modern Standard Arabic version of Wikipedia\n",
        "wiki2=wikipediaapi.Wikipedia(\"arz\") # arz refers to Egyption dialect version of Wikipedia"
      ],
      "metadata": {
        "id": "lRXo53-DbI4k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get pages based on their titles\n",
        "try:\n",
        "    page1 = wiki1.page('لهجة مصرية')\n",
        "    page2 = wiki2.page('اللغه المصريه الحديثه')\n",
        "    \n",
        "except:\n",
        "    print(\"An exception occurred\")"
      ],
      "metadata": {
        "id": "MTK_Km9NpzeB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the first 200 characters of the two articles\n",
        "print('Article 1:\\n',page1.summary[0:200])\n",
        "print('Article 2:\\n',page2.summary[0:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbMVGJMop3cb",
        "outputId": "8820f887-53e2-4d0a-be00-fa54cb3d5f2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            " اللهجة العربية المصرية، المعروفة محليًا باسم العامية المصرية أو المصري، يتحدث بها معظم المصريين المعاصرين.\n",
            "المصرية هي لهجة شمال إفريقية للغة العربية وهي فرع سامي من عائلة اللغات الإفروآسيوية. نشأت في \n",
            "Article 2:\n",
            " اللغه المصريه الحديثه او المصرى هى اللغه اللى المصريين بيتكلّموها فى مصر, ابتدا تاريخها فى دلتا النيل حوالين بلادها الحضريه زى القاهره واسكندريه.\n",
            "النهارده اللغه المصرى هى اللغه السايده فى مصر وبتتكون \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract text from the two pages\n",
        "article1=page1.text\n",
        "article2=page2.text\n",
        "text=[article1,article2]"
      ],
      "metadata": {
        "id": "WNUV5syOqNr0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special characters, punctuations, english letters, and digits removing"
      ],
      "metadata": {
        "id": "xF-gw_LaavVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text=[]\n",
        "for article in text:\n",
        "    article=re.sub(r'[^\\w\\s]','',article)   #Remove special characters & punctuations\n",
        "    article=re.sub(r'[a-zA-Z\\d]','',article) #Remove english letters & digits\n",
        "    cleaned_text.append(article)"
      ],
      "metadata": {
        "id": "dyf2zEecqVY4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display first 300 letters of the two articles\n",
        "print('Article 1:\\n',cleaned_text[0][0:300])\n",
        "print('Article 2:\\n',cleaned_text[1][0:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX9P1gjgqVlu",
        "outputId": "aea01cb6-ca9c-4d5b-e6d3-4f8b8c6e6033"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            " اللهجة العربية المصرية المعروفة محليا باسم العامية المصرية أو المصري يتحدث بها معظم المصريين المعاصرين\n",
            "المصرية هي لهجة شمال إفريقية للغة العربية وهي فرع سامي من عائلة اللغات الإفروآسيوية نشأت في دلتا النيل في مصر السفلى حول العاصمة القاهرة تطورت اللهجة المصرية من اللغة العربية التي تم نقلها إلى مصر \n",
            "Article 2:\n",
            " اللغه المصريه الحديثه او المصرى هى اللغه اللى المصريين بيتكلموها فى مصر ابتدا تاريخها فى دلتا النيل حوالين بلادها الحضريه زى القاهره واسكندريه\n",
            "النهارده اللغه المصرى هى اللغه السايده فى مصر وبتتكون من لهجات زى مصرى قاهراوى  بحيرى او مصرى دلتاوى فى جنوب مصر الناس بتتكلم مصري صعيدى و هى لغه بنفسيها معت\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenizing & Stopwords Removing\n",
        "\n"
      ],
      "metadata": {
        "id": "e53odOhvbpKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Stopwords text file from: https://github.com/mohataher/arabic-stop-words\n",
        "#Read the file\n",
        "stopwords=[]\n",
        "with open('/gdrive/MyDrive/ColabNotebooks/list.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "#remove \\n from each line\n",
        "for i in range(len(lines)):\n",
        "  stopwords.append(lines[i].rstrip('\\n'))\n",
        "#Print some of the stopwords\n",
        "print(stopwords[100:110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GmvqsiTMlQA",
        "outputId": "9bdc97c8-ab83-4eaa-efa0-8487891a84d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إليكم', 'إليكما', 'إليكنّ', 'إليكَ', 'إلَيْكَ', 'إلّا', 'إمّا', 'إن', 'إنَّ', 'إى']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Each article will be tokenized and stopwords will be reomoved\n",
        "filtered_tokens=[]\n",
        "filtered_articles=[]\n",
        "for article in cleaned_text:\n",
        "    #Tokenize\n",
        "    tokens=article.split()\n",
        "    #Check if any word is a stopword, to be neglected\n",
        "    for word in tokens:\n",
        "        if word not in stopwords:\n",
        "            filtered_tokens.append(word)\n",
        "    #Add tokenized article to the article list\n",
        "    filtered_articles.append(filtered_tokens)\n",
        "    filtered_tokens=[]"
      ],
      "metadata": {
        "id": "SyX67-Jnb1ay"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of the egyption article after tokenizing and stopwords removing\n",
        "print(filtered_articles[1][0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L7OHNoVdDZz",
        "outputId": "086c1e28-5361-415c-de73-5a69f2c23a7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['اللغه', 'المصريه', 'الحديثه', 'المصرى', 'هى', 'اللغه', 'اللى', 'المصريين', 'بيتكلموها', 'مصر', 'ابتدا', 'تاريخها', 'دلتا', 'النيل', 'حوالين', 'بلادها', 'الحضريه', 'زى', 'القاهره', 'واسكندريه']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-Speech and Named entity recognition tagging using CAMeL tools"
      ],
      "metadata": {
        "id": "y9EmgSCEhjYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POS Tagging"
      ],
      "metadata": {
        "id": "2IFA55rNfoYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ALERT: THIS CELL IS NOT WORKING\n",
        "#The issue only appears on Colab and it seems to be that a Google Drive is blocking the download since the file is too big and thus the antivirus cannot be applied:\n",
        "#FileNotFoundError: [Errno 2] No such file or directory: '/gdrive/MyDrive/camel_tools/data/morphology_db/calima-egy-r13/morphology.db'\n",
        "\n",
        "#Pretrained tagger for MSA \n",
        "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "#Pretrained tagger for Egyption Arabic \n",
        "mle2 = MLEDisambiguator.pretrained('calima-egy-r13')\n",
        "\n",
        "tagger = DefaultTagger(mle, 'pos')\n",
        "tagger2 = DefaultTagger(mle2, 'pos')\n",
        "\n",
        "pos_tags_article1 = tagger.tag(filtered_articles[0])\n",
        "pos_tags_article2 = tagger2.tag(filtered_articles[1])\n",
        "\n",
        "#Print part of POS tags for MSA article\n",
        "print(pos_tags_article1[0:20])\n",
        "\n",
        "#Print part of POS tags for Egyption article\n",
        "print(pos_tags_article2[0:20])"
      ],
      "metadata": {
        "id": "jOtiGtWrc2vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NER Tagging"
      ],
      "metadata": {
        "id": "BKk20AJguMt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition tags\n",
        "#Pretrained model\n",
        "ner = NERecognizer.pretrained()\n",
        "#Tags\n",
        "ner_tags=[]\n",
        "for article in filtered_articles:\n",
        "  labels = ner.predict_sentence(article)\n",
        "  ner_tags.append(list(zip(article, labels)))"
      ],
      "metadata": {
        "id": "qRguv6Ryng3o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of token-NER label pairs for article 1\n",
        "print('Article 1 NER:\\n',ner_tags[0][0:20])\n",
        "#Print part of token-NER label pairs for article 2\n",
        "print('Article 2 NER:\\n',ner_tags[1][0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqPmyEN1s0Nv",
        "outputId": "95eb61a8-9272-41f6-8f42-7162c1bc85ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1 NER:\n",
            " [('اللهجة', 'O'), ('العربية', 'O'), ('المصرية', 'O'), ('المعروفة', 'O'), ('محليا', 'O'), ('العامية', 'O'), ('المصرية', 'O'), ('المصري', 'O'), ('يتحدث', 'O'), ('معظم', 'O'), ('المصريين', 'O'), ('المعاصرين', 'O'), ('المصرية', 'O'), ('لهجة', 'O'), ('إفريقية', 'O'), ('للغة', 'O'), ('العربية', 'O'), ('فرع', 'O'), ('سامي', 'O'), ('عائلة', 'O')]\n",
            "Article 2 NER:\n",
            " [('اللغه', 'O'), ('المصريه', 'O'), ('الحديثه', 'O'), ('المصرى', 'O'), ('هى', 'O'), ('اللغه', 'O'), ('اللى', 'O'), ('المصريين', 'O'), ('بيتكلموها', 'O'), ('مصر', 'B-LOC'), ('ابتدا', 'O'), ('تاريخها', 'O'), ('دلتا', 'O'), ('النيل', 'I-LOC'), ('حوالين', 'O'), ('بلادها', 'O'), ('الحضريه', 'O'), ('زى', 'O'), ('القاهره', 'B-LOC'), ('واسكندريه', 'B-LOC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Find related open class words"
      ],
      "metadata": {
        "id": "8g15GCTEiN-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing Alef & Alef_maksura & teh_marbuta\n",
        "\n",
        "MSA applies Hamza of Alef hamza, teh marbuta, but Egyption Arabic does not use teh marbuta and no Hamza for Alef. The normalization is done to make the words more similar "
      ],
      "metadata": {
        "id": "Twlhk3MPTnI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize for Standard Arabic\n",
        "norm_article=[]\n",
        "norm_articles=[]\n",
        "\n",
        "for word in filtered_articles[0]:\n",
        "  #Normalize alef variants to 'ا'\n",
        "  word_norm = normalize_alef_ar(word)\n",
        "  #Normalize teh marbuta 'ة' to heh 'ه'\n",
        "  word_norm = normalize_teh_marbuta_ar(word_norm)\n",
        "  norm_article.append(word_norm)\n",
        "\n",
        "#Add the article after normalization to the list of normalized articles\n",
        "norm_articles.append(norm_article)\n",
        "norm_article=[]\n",
        "\n",
        "#Normailze for Egyption Arabic\n",
        "for word in filtered_articles[1]:\n",
        "  #Normalize alef maksura 'ى' to yeh 'ي'\n",
        "  word_norm = normalize_alef_maksura_ar(word)\n",
        "  norm_article.append(word_norm)\n",
        "\n",
        "#Add the article after normalization to the list of normalized articles\n",
        "norm_articles.append(norm_article)"
      ],
      "metadata": {
        "id": "-11HKy2WqGYp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the first 10 words in each article before and after normalization to show the difference\n",
        "print('Article 1:\\n',filtered_articles[0][0:10])\n",
        "print(norm_articles[0][0:10])\n",
        "print('*'*60)\n",
        "print('Article 2:\\n',filtered_articles[1][0:10])\n",
        "print(norm_articles[1][0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yz8CrBJlrbi",
        "outputId": "bb2a40ee-800c-4ca1-df17-1c4e10985335"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            " ['اللهجة', 'العربية', 'المصرية', 'المعروفة', 'محليا', 'العامية', 'المصرية', 'المصري', 'يتحدث', 'معظم']\n",
            "['اللهجه', 'العربيه', 'المصريه', 'المعروفه', 'محليا', 'العاميه', 'المصريه', 'المصري', 'يتحدث', 'معظم']\n",
            "************************************************************\n",
            "Article 2:\n",
            " ['اللغه', 'المصريه', 'الحديثه', 'المصرى', 'هى', 'اللغه', 'اللى', 'المصريين', 'بيتكلموها', 'مصر']\n",
            "['اللغه', 'المصريه', 'الحديثه', 'المصري', 'هي', 'اللغه', 'اللي', 'المصريين', 'بيتكلموها', 'مصر']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are similar words?\n",
        "I can answer this in two ways:\n",
        "\n",
        "**First: Synonyms**. Synonyms cannot be found using the existing Arabic NLP libraries -to my knowledge-, but we can achieve this task using a pre-trained embedding model or train any embedding model from scratch after transforming the Arabic text into embeddings (vector of numbers), then allow the model to calculate the distance between the words to find most similar words. I did an expriment using a pre-trained word2vec embedding model that was trained using Wikipedia Arabic articles. Unfortunately, all the similar words the model found were different inflections of words, not synonyms. \n",
        "\n",
        "**Second: Inflections**. Inflection is a process of word formation. We can return words to their root or lemma using lammtizating or stemming, then try to find similar roots between the two articles. This is faster than using embedding models.\n"
      ],
      "metadata": {
        "id": "uQkr6qEtilHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Since POS tagging didn't work on Colab, thus, I didn't use NER tags either to find similar words. I used an Arabic stemmer in NLTK library since there is not any stemmer in CAMeL tools.**"
      ],
      "metadata": {
        "id": "_flLWwM6k99q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming to find similarities\n"
      ],
      "metadata": {
        "id": "9KegjPpxmvZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the stemmer\n",
        "stemmer = ISRIStemmer()\n",
        "similar_pairs=[]\n",
        "article1=list(set(norm_articles[0]))\n",
        "article2=list(set(norm_articles[1]))\n",
        "for i in range(0,len(article1)):\n",
        "  for w in range(0,len(article2)):\n",
        "    if stemmer.stem(article1[i]) == stemmer.stem(article2[w]):\n",
        "      similar_pairs.append([i,w])"
      ],
      "metadata": {
        "id": "45luR28dptcu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print no. of similar pairs\n",
        "print(len(similar_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mizgTRmXvNbG",
        "outputId": "96d9ee65-69a3-4071-d4ae-b9555da17e4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of the similar pairs\n",
        "print(similar_pairs[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdJdeUk5vh2Z",
        "outputId": "bd69af36-5f83-4957-8e71-faeedef69000"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 1306], [3, 1061], [4, 3], [4, 205], [4, 308], [4, 566], [4, 742], [4, 896], [4, 1119], [4, 1244]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print part of the similar words between the two articles\n",
        "print(\"Article 1 _______ Article 2\")\n",
        "for i in range(30,60):\n",
        "  print(article1[similar_pairs[i][0]],' _______ ',article2[similar_pairs[i][1]])\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r6e0HOLzBp7",
        "outputId": "bc576f12-7527-4b56-b3e3-04dd46bb2c32"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1 _______ Article 2\n",
            "بطريقه  _______  الطريق\n",
            "بطريقه  _______  بطريقة\n",
            "المصريه  _______  المصريه\n",
            "المصريه  _______  مصريه\n",
            "المامور  _______  امريكا\n",
            "المامور  _______  أمر\n",
            "المامور  _______  الامر\n",
            "المامور  _______  امير\n",
            "وعلمانيا  _______  العلمانيين\n",
            "وعلمانيا  _______  بيتعلم\n",
            "وعلمانيا  _______  اتعلمتها\n",
            "وعلمانيا  _______  معلومات\n",
            "وعلمانيا  _______  التعليم\n",
            "وعلمانيا  _______  علمانيه\n",
            "وعلمانيا  _______  تعليم\n",
            "وعلمانيا  _______  اتعلم\n",
            "وعلمانيا  _______  الاعلام\n",
            "وعلمانيا  _______  العالم\n",
            "وعلمانيا  _______  العالمي\n",
            "وعلمانيا  _______  بيعلم\n",
            "وعلمانيا  _______  العلميه\n",
            "وعلمانيا  _______  الاعلاميين\n",
            "وعلمانيا  _______  التعليمي\n",
            "وعلمانيا  _______  علما\n",
            "الطعم  _______  طعيمة\n",
            "ظاهر  _______  ظاهرة\n",
            "ظاهر  _______  لظاهرة\n",
            "ظاهر  _______  الظاهرة\n",
            "تمثل  _______  مثلها\n",
            "تمثل  _______  بتمثل\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These 3 cells we run in first time using this notebook on Colab only:"
      ],
      "metadata": {
        "id": "jRKIfPYo0Uic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-tools -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "xItbltssY2Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-tools -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%mkdir /gdrive/MyDrive/camel_tools"
      ],
      "metadata": {
        "id": "R43D9c_g0up8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install camel tools data\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n",
        "#!export | camel_data -i all\n",
        "!export | camel_data -i defaults"
      ],
      "metadata": {
        "id": "plbhDr1O0wg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "34tb5DWDiZaS",
        "jRKIfPYo0Uic"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}